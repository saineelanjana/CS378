{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZTQoQfZr6af"
   },
   "source": [
    "# CS 378 Homework 1: Machine Learning on the ProPublica COMPAS Dataset (100 pts)\n",
    "\n",
    "## Deadline: 11:59 pm, September 7, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gfy4Q-Dr6ak"
   },
   "source": [
    "Please submit this completed notebook file to Canvas once finished. For policies regarding extensions and collaboration/honesty, please see the course syllabus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbsAddZsr6am"
   },
   "source": [
    "## Overall goals\n",
    "Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is a proprietary AI tool that some U.S. courts use to estimate the risk of recidivism in a defendant. The nonprofit organization Propublica peformed an analysis of COMPAS and found it to systemically discriminate against black defendants. In this assignment, you will use a dataset of COMPAS scores (provided as part of this assignment) to reproduce parts of Propublica's analysis, and also train your own ML-based criminal recidivism predictors. \n",
    "\n",
    "Before you start working on this assignment, please read [this article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), which describes Propublica's findings. You can also explore Propublica's methodology [here](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm). \n",
    "\n",
    "\n",
    "## Loading and surveying the data\n",
    "\n",
    "You can download the dataset for this assignment as a CSV file from [here](https://drive.google.com/file/d/1e8vyCBn8u2J2s5EVUPSCm7zkI1d5CkFc/view?usp=sharing). Please make sure you have the csv file in the same directory as this Python notebook to load the data.\n",
    "\n",
    "In order to get started, we first need to load our dataset into the code. We do this using a popular Python data science framework called PANDAS. First, we load our dataset (compas-scores-two-years.csv) into Python as a PANDAS dataframe. The .head() command gives the first 5 entires so that we can get a peek of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "XdqajAVkr6an",
    "outputId": "fc920597-81b0-49dd-bbe2-2cf93a0bebf6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # muting a trivial warning about PANDAS, don't worry about this\n",
    "df = pd.read_csv('compas-scores-two-years.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAOZiNHJr6ap"
   },
   "source": [
    "Then, we look into the kind of data the dataset contains by calling the columns field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvvtgUd5r6aq",
    "outputId": "e797eb31-943f-4227-f386-4863b19af17f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9Wlwy02r6ar"
   },
   "source": [
    "The shape field gives us how many rows and columns the dataset has (therefore the name shape). We will constantly use this command to check how we are doing regarding data cleaning and dataset manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxBPnwZLr6as",
    "outputId": "d9de63e2-e0e4-4485-d281-2885603ef5b3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXMFKrptr6at"
   },
   "source": [
    "## Question 1: Data Cleaning (3 points)\n",
    "Now we clean the data. This data-cleaning is largely based off of ProPublica's methods. First, we only focus on cases where the COMPAS scored crime happened within 30 days from when the person was arrested. Then, we also get rid of cases where is_recid is -1 since we only want binary values for the purpose of our model (0 for no recidivism, 1 for yes recidivism). Finally, we don't want the c_charge_degree to be \"O\" which denotes ordinary traffic offenses (not as serious of a crime), and we don't want the score_text to be \"N/A\". All of this is done using the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzifEkt3r6aw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.loc[(df['days_b_screening_arrest'] <= 30) & (df['days_b_screening_arrest'] >= -30) \n",
    "              & (df['is_recid'] != -1) & (df['c_charge_degree'] != \"O\") & (df['score_text'] != 'N/A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ewo-vzRNr6ax",
    "outputId": "d33ac919-de53-4da4-a9db-534bc905851d"
   },
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hoy73bUVr6ay"
   },
   "source": [
    "Now we choose which columns to pick. Notice that by not picking names we are able to anonymize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "sAqeOzYUr6a0",
    "outputId": "292cac9b-a308-4856-9c2f-f625d9aa9dbd"
   },
   "outputs": [],
   "source": [
    "df_filtered = df_cleaned[['age','sex', 'race', 'juv_fel_count', 'decile_score', 'priors_count', 'is_recid', 'is_violent_recid', \n",
    "                   'v_decile_score']]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IB4uYl_jr6a1",
    "outputId": "d8d49da3-4b04-4dc6-f700-98425721ce56"
   },
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1:  Look at the original dataset and the cleaned one (df_filtered). Pick one column (aside from name) that was deleted and one column that wasn't and provide justifications for why they were deleted / why they were not. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1pZR2pDr6a1"
   },
   "source": [
    "One final data manipulation we need to do is on the race column. Notice that for the race column, we have strings as our race descriptions. If we want to use it as an input to our model later on, it needs to be a numeric value. Therefore, we create additional columns and use them as indicator random variables (1 denoting that the row belongs to the race and 0 denoting that the row doesn't). In addition, we replace the (binary) sex field by 0/1: 1 for male and 0 for female.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "vMj4wwV4r6a2",
    "outputId": "d3fb9c2a-f915-4655-e4f0-d0f457e6c933"
   },
   "outputs": [],
   "source": [
    "df_final = df_filtered.join(pd.get_dummies(df_filtered['race']))\n",
    "df_final[\"sex\"] = (df_final[\"sex\"] == \"Male\") + 0 ## Use the binary coding for sex.\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the final shape of our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76GC7d8lr6a3",
    "outputId": "49c22a03-00e7-4ba3-d88b-e25b45535878",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqM7du9Qr6a4"
   },
   "source": [
    "## Question 2: Data Analysis & Visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cleaned, filtered out dataset -- df_final -- to work with. From now on, we are going to work with this dataset unless specified otherwise. \n",
    "\n",
    "Before we start doing machine learning, we will perform some manual analysis and visualization of the data. We start by importing all the libraries we need for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IQcWK0Jr6a5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkQ8dDkXr6a6"
   },
   "source": [
    "We first want to explore some basic summary satistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "ywnCgWRfr6a7",
    "outputId": "248a5da7-f349-4731-9c26-fb1ffe6766d5"
   },
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVrW9g_Rr6a8"
   },
   "source": [
    "#### Question 2.1: Using the df_final dataset, compute the means of criminal recidivism ('is_recid') for white males, white females, black males, and black females, as well as the general population. Also, construct histograms that visualize the distribution of 'is_recid' for these subgroups and the general population. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBP5Y81mr6a9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2: Repeat the analysis above for decile_score and violent_decile_score. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3: Share any insights about the data that you may have developed from the above visualizations. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzQZE_our6bY"
   },
   "source": [
    "## Question 3: Replicating ProPublica's Analysis (12 points)\n",
    "Now that we have a much more comprehensive understanding of the dataset after loading, cleaning, analyzing, and visualizing it, we reproduce the threshold rule analysis ProPublica has conducted. As a recap, Propublica used the COMPAS scores to predict recidivism if the score was >=5 and no recidivism if the score was < 5.\n",
    "\n",
    "Note that this is not complete since it solely uses the decile score and does a hard thresholding for prediction, discarding all other aspects of individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTD7W43Hr6bY"
   },
   "source": [
    "### Filtering the dataset using race\n",
    "\n",
    "Now we use a filtering operation to select the rows for everyone in the African-American population. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kt8dPWir6bZ"
   },
   "outputs": [],
   "source": [
    "df_black = df_final[df_final.race == \"African-American\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OxmFF5Zr6bZ"
   },
   "source": [
    "Take a look into the dataframe we just got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "hz9dQOcKr6ba",
    "outputId": "2f138746-7725-4f89-d906-7d1080f4c7e2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKJvQ4t4r6bd"
   },
   "source": [
    "### A simplified thresholding rule\n",
    "Now, let's use a simple thresholding rule to \"predict\" recidivism, in the spirit of ProPublica's analysis: for `decile_score >= 5`, predict `recidivism = True`; and for `decile_score < 5`, predict `recidivism = False`. We save our prediction to the column `predicted_recid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU2s1xnfr6be"
   },
   "outputs": [],
   "source": [
    "df_black[\"predicted_recid\"] = (df_black.decile_score >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "PsVvCYonr6be",
    "outputId": "de5d4175-a740-4a5a-fe7d-d154cd96d16b"
   },
   "outputs": [],
   "source": [
    "df_black.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1: Using the sklearn package, construct and visualize the confusion matrices for the entire population, the black population, and the white population. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2: Compute the accuracy, precision, recall, false positive rate, and false negative rate for the entire population, the white subpopulation, and the black subpopulation. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Machine Learning (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to the actual machine learning. As mentioned in class, we first define our features `X`, which we use to predict, and the label `Y`, which we try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop(columns=['is_recid', 'is_violent_recid', \"race\"])\n",
    "Y = df_final['is_recid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.1: Explain why we are dropping is_violent_recid and race. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide the dataset into training / testing parts. We will use the training dataset to train our model to make predictions, in this case criminal recividism. Then, we will use the testing set to see how our model performed. A 80:20 split is pretty standard in practice. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `random_state = 155` sets a random seed for the splitting, so that everytime you run the above code, you will end up with the exact same split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "into the shape of train_x, test_x, train_y, and test_y using our favorite shape function. If all has been implemented correctly, it should be (4937, 12), (1235, 12), (4937,), and (1235,). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.2: Write code to construct test datasets (test_x_w, test_y_w) and (test_x_b, test_y_b) corresponding to the white and black individuals in the test set, respectively. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Now we will experiment with multiple models that we learned about in class and compare their performance. We start with logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.3: Train a Logistic Regression model on the data (this dataset should include individuals of all races). Aim to choose hyperparameters (see documentation) such that the model performs the best and behaves the most equitably! Report on the following metrics: (i) Training accuracy; (ii) Test accuracy, precision, recall, false positive rate, and true positive rate for the overall population;  (iii) Test accuracy, precision, recall, false positive rate, and true positive rate for the white population; (iv) Test accuracy, precision, recall, false positive rate, and true positive rate for the black population; (v) The ROC curves for the black and white populations. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.4: Comment on the social implications, as you see them, of your results in Question 4.3. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.5: Train a neural network on the data. Aim to choose hyperparameters such as depth and width such that the model performs the best and behaves the most equitably. Compute and report on the metrics considered in Question 4.3, (i)-(v). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "#### Question 4.6: Train a decision tree classifier (of a suitable depth) on this dataset. As before, choose hyperparemeters so as to maximize performance and equity. Compute and report on the metrics considered in Question 4.3, (i)-(iv). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.7: Comment, with some empirical evidence, on how the performance and fairness of the model changes with the maximum tree depth. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "\n",
    "#### Question 4.8: Read up on random forest classifiers (https://en.wikipedia.org/wiki/Random_forest). Train a random forest and compute the metrics in Question 4.3, (i)-(iv), using your model. Relate your results with those for the decision tree model. (13 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.9: Write a few sentences comparing the performance and fairness/unfairness of the different models you trained in this task.  (6 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_mSlszqr6bx"
   },
   "source": [
    "## Question 5: Reflections on the case (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is graded based on completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.1: Having completed this assignment, what are your thoughts on the use of machine learning in sentencing procedures? For example, you could approach this question by listing some of the pros and cons of human vs. automated decision making in this setting. (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://github.com/propublica/compas-analysis/\n",
    "- https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
    "- https://pandas.pydata.org/\n",
    "- https://jupyter.org/\n",
    "- https://matplotlib.org/stable/index.html\n",
    "- https://seaborn.pydata.org/\n",
    "- https://numpy.org/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gQ1U5M3Pr6bE",
    "wd942zEor6bu",
    "Y_mSlszqr6bx"
   ],
   "name": "Homework 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
